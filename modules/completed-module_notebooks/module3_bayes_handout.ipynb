{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "    border-style: groove;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "    border-style: groove;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Module 3 - Naive Bayes\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "\n",
    "We will look at a competitor to K-NN this week, Naive Bayes. We will continue to work with the tweets so let's bring them in.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "week = '2'\n",
    "\n",
    "home_path =  os.path.expanduser('~')\n",
    "\n",
    "file_path = '/Documents/datascience_2/table_history/'\n",
    "\n",
    "file_name = 'tweet_table_w'+week+'.csv'\n",
    "\n",
    "tweet_table = pd.read_csv(home_path + file_path + file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/fickas/datascience_2\n",
      " * branch            master     -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "library_folder = '/Documents/datascience_2/code_libraries'\n",
    "os.chdir(home_path + library_folder)\n",
    "!git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_all_hashes\t do_nothing\t foo\t knn_voting\t pat_count\t python_version\t top_k\t \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(home_path + library_folder)\n",
    "from week2 import *\n",
    "from mylibrary import *  #I am keeping a private library of some functions\n",
    "%who function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>bang_count</th>\n",
       "      <th>hash_count</th>\n",
       "      <th>#trump</th>\n",
       "      <th>#politics</th>\n",
       "      <th>#allahsoil</th>\n",
       "      <th>#libtard</th>\n",
       "      <th>#liberal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so...</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  length  \\\n",
       "0   1      0  @user when a father is dysfunctional and is so...     101   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...     122   \n",
       "2   3      0                                bihday your majesty      19   \n",
       "3   4      0  #model   i love u take with u all the time in ...      84   \n",
       "4   5      0             factsguide: society now    #motivation      38   \n",
       "\n",
       "   bang_count  hash_count  #trump  #politics  #allahsoil  #libtard  #liberal  \n",
       "0           0           1       0          0           0         0         0  \n",
       "1           0           3       0          0           0         0         0  \n",
       "2           0           0       0          0           0         0         0  \n",
       "3           3           1       0          0           0         0         0  \n",
       "4           0           1       0          0           0         0         0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Another method: Naive Bayes\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "The problem is still the same. Given a tweet, predict whether it is hate or nohate. The twist is that I am going to reform the question in terms of probabilities using a method called *Naive Bayes*. I'll tell you what is naive about it later. Fow now, you need to do a perspective shift. We will no longer be using a table-based approach that looks at columns of features. Instead we will use counts of things. First, here is the formula we want to compute.\n",
    "<p>\n",
    "<img src='https://www.dropbox.com/s/gstzvvtvh9b39o8/bayes.png?raw=1'>\n",
    "<p>\n",
    "The way we will use this formula is as follows:\n",
    "<p>\n",
    "<ol>\n",
    "<p>\n",
    "<li>We have a tweet T with zero or more hashtags. We want to know whether to mark it as hate (1) or nohate (0).</li>\n",
    "<p>\n",
    "<li>We compute P(nohate|hashtags). What is the probability that T is nohate given its hashtags.</li>\n",
    "<p><li>We compute P(hate|hashtags). What is the probability that T is hate given its hashtags.</li>\n",
    "<p><li>We compare the 2 probabilities. The higher probability wins and we use the associated value for prediction.</li>\n",
    "</ol>\n",
    "<p>\n",
    "In summary, we always compute the probability for all possible prediction values. We only have two, hate and nohate, so we always compute two. If you had three possible values (next week), you would compute three probabilities, etc.\n",
    "<p>\n",
    "We read the left hand side as \"The probability of value O (hate or nohate) given the evidence which we supply as the hashtags of the tweet. For the right hand side, the notation is as follows:\n",
    "<ul>\n",
    "<p>\n",
    "<li>O stands for one of the prediction values. In our case 0 (nohate) or 1 (hate).</li>\n",
    "<p><li>E1 ... En stands for the evidence we have for making prediction O. I am going to use the #hashtags we find in the tweet as the evidence. So E1 could be #trump, E2 could be #cnn, etc.</li>\n",
    "<p><li>P(E1|O) stands for the probability we see E1 (one of the hashtags in the tweet) associated with the case O. This ends up being the total number of times we see E1 with O divided by the total number of O tweets. If the hashtag #trump appears in every hate tweet, then P(#trump|hate) = 1. If it never appears in a hate tweet, then P(#trump|hate) is 0.</li>\n",
    "<p><li>P(O) is what percentage of tweets that are case O.</li>\n",
    "<p><li>P(E) = P(E1) `*` P(E2) ... `*` P(En). P(E1) is how many tweets does E1 appear in divided by the total number of tweets. Ditto for P(E2), etc. But guess what? We are going to ignore the denomominator! Wait. You can't just throw out the denominator. But we can. You will see why shortly.\n",
    "</ul>\n",
    "<p>\n",
    "There is a special case we need to deal with. What if the tweet we are trying to predict on has no hashtags? Then we have something that looks like `P(O|)`. Kind of weird. I am going to define that as evidence E0, i.e., P(O|E0).\n",
    "<p>\n",
    "Let's think a bit about this. What data do we need to compute the various components of the formula:\n",
    "<ul>\n",
    "<p>\n",
    "<p><li>For P(E1|O) we need to know how many times the hashtag appears in nonhate tweets and how many times it appears in hate tweets. We can add those to get the total number of tweets it appears in.</li>\n",
    "<p><li>For P(O) we need the count of each type of tweet, nohate and hate. And we can add those to get the total number of tweets.</li>\n",
    "<p><li>For P(E0|O) we need the count of nohate tweets that have no hashtag and the count of hate tweets that have no hashtag. And we can add those to get the total.\n",
    "</ul>\n",
    "<p>\n",
    "The good news is that we have a large part of this from week 2. That's why I asked you to build the somewhat odd dictionary `all_hashes` in week 2. It contains the info we need for P(Ei|O). Let's bring that in now and get it set up.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Bring over count_all_hashes from week 2\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Reminder: we need a count on all hashtags broken out by how many times each appears in a nohate tweet and how many times in a hate tweet. So a dictionary with keys as hashes and values as a 2 place list, e.g., `{'#foo': [2, 5], 'fum': [23, 0], ...}.` In this example, we can see that the hash #foo appears 2 times in a nohate tweet and 5 times in a hate tweet.\n",
    "<p>\n",
    "You should bring over the `count_all_hashes` function from week 2. And fill it in and execute it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function mylibrary.count_all_hashes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def count_all_hashes(table):\n",
    "# already imported \n",
    "count_all_hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_hashes = count_all_hashes(tweet_table)  # this gives us info on P(Ei|O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24084"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 133]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hashes['#trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 1 - we need some basic probabilities\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "We will need various counts and probabilities. Please calculate the values and put them in useful_counts. And don't just copy my values as constants!\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_count': [29720, 2242],\n",
       " 'class_prob': [0.92985420186471435, 0.070145798135285653],\n",
       " 'tweet_count': 31962}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For P(E1|O) we need to know how many times the hashtag appears in nonhate tweets and how many times it appears in hate tweets. We can add those to get the total number of tweets it appears in.\n",
    "# For P(O) we need the count of each type of tweet, nohate and hate. And we can add those to get the total number of tweets.\n",
    "# For P(E0|O) we need the count of nohate tweets that have no hashtag and the count of hate tweets that have no hashtag. And we can add those to get the total.\n",
    "\n",
    "useful_counts = {}\n",
    "\n",
    "useful_counts['tweet_count'] = len(tweet_table)\n",
    "useful_counts['class_count'] = [len(tweet_table)-sum(tweet_table['label']) , sum(tweet_table['label'])]\n",
    "useful_counts['class_prob'] = [useful_counts['class_count'][0]/useful_counts['tweet_count'] , useful_counts['class_count'][1]/useful_counts['tweet_count']]\n",
    "\n",
    "useful_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 2 - E0 (AKA naked tweets)\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "We need a count of nohate tweets lacking a hashtag and hate tweets lacking a hashtag. Please put your info in `useful_counts`. And again, do your own calculation and don't just copy my constant values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_count': [29720, 2242],\n",
       " 'class_prob': [0.92985420186471435, 0.070145798135285653],\n",
       " 'naked_count': [7899, 607],\n",
       " 'tweet_count': 31962}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out tweets with hash_count == 0\n",
    "naked_tweets = tweet_table.loc[tweet_table['hash_count'] == 0]\n",
    "useful_counts['naked_count'] = [len(naked_tweets)-sum(naked_tweets['label']) , sum(naked_tweets['label'])]\n",
    "\n",
    "useful_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 3 - Let's put it together\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Ok, I think we are ready for our naive_bayes function. Please fill it out and make sure you match my results.\n",
    "<p>\n",
    "But one important point: if you find a hashtag that is not in all_hashes, just skip over it. I'll say something more about this later.\n",
    "<p>\n",
    "Another important note. We will be using this same function next week on a whole new problem looking at regular old sentences (no hashtags involved). And we will have 3 possible classes, i.e., 0,1,2. So try hard not to tie your function directly to the 2-class tweet problem. I know this is a bit of a problem given our notion of naked tweets will not carry over to week 4. But see if you can make other portions of your code general.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#y3s', '#YAYS', '#DYa', '#no']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_pat = r'[#][\\w]+' # hmm... need to go back and test this against module 2 still (and if it's good, update all_hashes..)\n",
    "\n",
    "# hash_pat = r'[#][\\W]+' # hmm...\n",
    "hash_patc = re.compile(hash_pat)\n",
    "\n",
    "hash_patc.findall(\"yeah#y3s#YAYS! + an#DYa #no$\") # just testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(text, count_dictionary, patc, bag_of_words, class_size):\n",
    "\n",
    "    ##\n",
    "    # --- Interpretation of variables in context of tweets: hate/nohate\n",
    "    # text = I want to predict a category/class for\n",
    "    # count_dictionary = useful counts obtained from a labled dataset table\n",
    "    # patc = regex pattern for searching (hashtags)\n",
    "    # bag_of_words = our table of all hashtags from the tweet_table\n",
    "    # class_size = number of possible categories (hate or nohate)\n",
    "    # NoHate = class 0 , Hate = class 1\n",
    "    #\n",
    "    # Will return list of probabilities for each class\n",
    "    ##\n",
    "    \n",
    "    from functools import reduce # need to use to computing a product over a list \n",
    "    import re # for regex processes\n",
    "        \n",
    "    ### From text, extract all the hashtags and store in 'hashtags'\n",
    "    patterns = re.compile(patc) # creates the regex pattern\n",
    "    hashtags = patterns.findall(text) # gets all the hashtags from the text\n",
    "    \n",
    "    \n",
    "    ### Check if it's a 'naked tweet' (no hashtags) --- Treat this as a Separate Case\n",
    "    if hashtags == []: # naked tweet\n",
    "        # Compute 'naked tweet' Class Probabilities\n",
    "        naked_prob = []\n",
    "        for i in range(0,class_size): # checks i=0,1,...,class_size-1\n",
    "            naked_prob = naked_prob + [count_dictionary['naked_count'][i] / count_dictionary['tweet_count']] \n",
    "            # 'naked_count' and 'class_count' are hard-coded in useful_counts \n",
    "            \n",
    "            # Note: at first I thought it might be \n",
    "            # naked_prob = naked_prob + [count_dictionary['naked_count'][i] / count_dictionary['class_count'][i]] \n",
    "            # but that skews the values by measuring the wrong proportions\n",
    "            \n",
    "        return naked_prob # the probabilities for different classes given a naked tweet\n",
    "    \n",
    "    else: ### the typical case\n",
    "    \n",
    "        ### Compute Background Class Probabilities: what will be P(nohate) - (OPTION 0) & P(hate) - (OPTION 1) \n",
    "        background_prob = []\n",
    "        for i in range(0,class_size): # checks i=0,1,...,class_size-1\n",
    "            background_prob = background_prob + [count_dictionary['class_prob'][i]] # 'class_prob' is hard-coded in useful_counts \n",
    "\n",
    "\n",
    "        ### Compute P(hashtags=evidence | Outcomes = classes)\n",
    "        likelihoods = [[1] * class_size] # initializes list of sublists [ [*], [*], ... ]\n",
    "        # these will hold the conditional probabilities P (Ei | O)\n",
    "        # each sublist [*] will hold the P(Ei | O) given the single hashtage Ei \n",
    "        # with an entry for all the different classes O - i.e., [*] = [P(\"trump\" | nohate) , P(\"trump\" | hate)]\n",
    "        # likelihoods initialized as [ [1,1,...,1] ]. Since we will multiply over it\n",
    "        # 'default information' = entry of 1 is fine, won't modify probabilities\n",
    "        # other option is initialize as empty.. but problems computing probabilities then\n",
    "        # this way we avoid edge case of no tweets from table.    \n",
    "\n",
    "        for entry in hashtags: # loop over hashtags\n",
    "\n",
    "            if entry not in bag_of_words:\n",
    "                pass # do nothing - we won't count a hashtag if it's not in our list yet\n",
    "            else: \n",
    "                new_probabilities = [] # initialize a sublist [*]\n",
    "                for i in range(0,class_size): # iterate over the classes\n",
    "                    new_probabilities = new_probabilities + [bag_of_words[entry][i] / count_dictionary['class_count'][i]]\n",
    "                    # \n",
    "\n",
    "                likelihoods.append(new_probabilities) #add a sublist of P(Ei|O) for fixed Ei and all classes O to likelihoods\n",
    "\n",
    "        ### Compute class probabilities given evidence: P(O | E1 ... Ek)\n",
    "        # we ignore P(E1 ... Ek) normalization factor since it's the same for both terms & so doesn't influence comparison\n",
    "        probabilities = [] \n",
    "\n",
    "        for i in range(0,class_size): # multiply all corresponding probabilities for each class\n",
    "            probabilities = probabilities + [ background_prob[i] * reduce(lambda x, y: x * y, [item[i] for item in likelihoods], 1) ]\n",
    "        # looking around online, we can probably slightly optimize above by using different function than reduce...\n",
    "\n",
    "        return probabilities    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0010324760653275765, 0.0]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes(tweet_table.loc[0, 'tweet'], useful_counts, hash_patc, all_hashes, 2)  # actual 0 - nohate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Wow. Pretty dang certain that the label is not 1 for row0.\n",
    "<p>\n",
    "Think a bit about how we could get a 0.0 value for hate. I'll bring the formula down here so we can study it. Fill in the O with hate.\n",
    "<p>\n",
    "<img src='https://www.dropbox.com/s/gstzvvtvh9b39o8/bayes.png?raw=1'>\n",
    "<p>\n",
    "If `P(hate)` was 0 that would do it. But seems kind of unlikely that we have zero hate tweets. That means one or more of the `P(hashtag|hate)` values was 0. In essence, one or more of the hashtags in row0 never appears in a hate tweet.\n",
    "<p>\n",
    "Let's try another.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.062648656237095e-12, 1.5685392010692217e-09]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes(tweet_table.loc[13, 'tweet'], useful_counts, hash_patc, all_hashes, 2)   # actual 1 - hate tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Got it right again.\n",
    "<p>\n",
    "Let's look at the problem of seeing unknown hashtags, i.e., ones not in all_hashes. We can test our code out to make sure things are handled correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'#stevef' in all_hashes  # so #stevef is new tag not seen previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "I said to skip over unknown hashtags so we should skip over #stevef.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.92985420186471435, 0.070145798135285653]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = naive_bayes('here is a new tweet by #stevef', useful_counts, hash_patc, all_hashes, 2) # skip unknown but don't treat as naked tweet\n",
    "\n",
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Above is just (P(nohate), P(hate)) which is same as general class probabilities.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 == useful_counts['class_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Ok, let's test it with a naked tweet - no hashtags.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24713722545522809, 0.018991302171328453]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = naive_bayes('here is a new tweet by stevef', useful_counts, hash_patc, all_hashes, 2)  #here is naked tweet\n",
    "test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Looks like now treating it as a naked tweet and rightfully so.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test2 == tuple(1.0*x/useful_counts['tweet_count'] for x in useful_counts['naked_count'])\n",
    "test2 == [1.0*x/useful_counts['tweet_count'] for x in useful_counts['naked_count']]\n",
    "\n",
    "\n",
    "# note for myself:\n",
    "# initially I measured this for the 'alternate' normalization factors (which were wrong) \n",
    "# test2 == [1.0*useful_counts['naked_count'][i]/useful_counts['class_count'][i] for i in list(range(len(useful_counts['class_count'])))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "I am saying that a tweet with a hashtag is not a naked tweet, even if we do not recognize the hashtag. Seems reasonable to me. But more generally, my choice of skipping over unknowns is semi-arbitrary. I'll give you another approach later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Ok, I think we can do this quickly\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "But I'll still time it just in case. I want the predictions for all the tweets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "did 1000\n",
      "2.4525539875030518\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "predictions = []\n",
    "for i,row in tweet_table.iterrows():\n",
    "    if i%1000 == 0: print('did 1000')\n",
    "    pair = naive_bayes(row['tweet'], useful_counts, hash_patc, all_hashes, 2)\n",
    "    predictions.append(0 if pair[0] >= pair[1] else 1)\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)  # in seconds\n",
    "\n",
    "## about 2.5 seconds... ~3x as fast as yours? I'm a little worried at that kind of speedup..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Don't know about you, but I think we won big\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I think pre-building the hashtag counts dictionary gave us a huge speed-up. 7 seconds for 32K tweets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Challenge 4 - compute the confusion matrix\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Generate the confusion matrix (as a dictionary) so we can check out how we are doing. And once again, do not simply copy my values - do your own computation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actuals = tweet_table['label']  #easy peasy to pull a column out of a table\n",
    "zipped = zip(predictions,actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 29631, (0, 1): 697, (1, 0): 89, (1, 1): 1545}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_dictionary = {(1,1):0, (1,0):0, (0,1):0, (0,0):0}\n",
    "for pair in zipped:\n",
    "    confusion_dictionary[pair] += 1\n",
    "confusion_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Kind of interesting at first glance\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "We are making 90 errors on predicting hate when it is not. But 703 errors predicting non-hate when it was hate.\n",
    "<p>\n",
    "How should we feel about that? If you are a free-speecher, you might be happy. We are erring on side of letting things through. If you are a parent, you might prefer more (1,0) than (0,1).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754082973531069"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (1.0*confusion_dictionary[(0,0)]+confusion_dictionary[(1,1)])/len(tweet_table)\n",
    "accuracy\n",
    "\n",
    "# compare against your 0.9751892872786434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "Let's check this against what we would get if we always predicted 0 (nohate).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92985420186471435"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_counts['class_prob'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "Not bad. We jumped 5 percent. And this is a hard problem because there are so few hate tweets in the overall set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Why can we throw away the denominator?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Our prediction rule is calculate P(nohate|E1, E2, ...Ek) and P(hate|E1, E2, ...Ek). Then choose the larger. Using Naive Bayes theorem we see that the denominator P(E1) `*` P(E2) ... `*` P(Ek) is same for both; the denominator does not depend on the case we are looking at. It will not affect the outcome. Throw it out.\n",
    "<p>\n",
    "BTW: I would argue along a similar line for euclidean distance. Suppose I have ed(v0,v1) &lt; ed(v0,v2). The last thing we do in the ed function is take the square root. What if I eliminated that step. Would it change the outcome of the comparison? Nope.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Why is Bayes Naive?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Well, Bayes, himself, was not naive. Or maybe he was, don't know. What is meant by naive is an assumption we make to allow us to use the simplified formula we used above.\n",
    "Let's look again at row13 from week 2. The non-naive way of looking at it is `P(hate, #cnn, #michigan, #tcot)` where we view all terms as dependent on each other, e.g. we can't view #cnn on its own but only in conjunction with (conditional on) #michigan and #tcot. The probability-chain rule formula captures this idea:\n",
    "<p>\n",
    "<img src='https://www.dropbox.com/s/v2ppatadfghvrb4/chain2.png?raw=1'>\n",
    "<p>\n",
    "All of these probabilities are known to us so we could calculate an answer. Our current approach tries to avoid calculating much at prediction time. Instead we pre-build our dictionary and store the counts there. Computation cost is just look up cost in a Python dictionary (fast). Can we use the same approach with the formula above? We would need a dictionary that contains all possible combinations of hashtags with their counts, i.e., the powerset. We have 23K unique hastags. The size of the powerset is `2**N`. So we are looking at a dictionary with 2**23000 items. Gulp. That's not going to happen. Our alternative is to forget the dictionary and just do on-demand counting for each new tweet and set of hashtags. But this could be slow if we have to search the entire tweet_table multiple times for every new tweet. And if you are an app developer with a cool new tweet-filter, you want this happening in real-time as tweets come in.\n",
    "<p>To get around the space and computational costs of the formula above (the full chain rule), Smith said let's pretend that all terms are independent of each other. Others said, don't be naive, Smith, you can't assume that terms are totally independent. But Smith persevered. And she was right, the naive assumption does work well in many cases. So Naive Bayes is normal Bayes but with the strong (naive) assumption that all items are independent.\n",
    "<p>\n",
    "Who was Smith? I don't have a definitive answer. But I like her thinking. Try simple first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Can we find an alternative to skipping unknowns?\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Someone can always come up with an alternative in probabilistic reasoning :)\n",
    "<p>\n",
    "Let's say we don't skip unknown hashtags. So we treat things like `#stevef` as probability 0 for both cases (!) Of course, this would lead to a probability of nohate as 0 and a probability of hate as 0. Not so good. To get around that, we add a smoothing factor to P(Ei|O). \n",
    "<p>\n",
    "<img src='https://www.dropbox.com/s/ejp4nm1y3uldax2/smoothng2.png?raw=1'>\n",
    "<p>\n",
    "The # symbol stands for count above. The value of k is your choice. In fact, you could experiment with different values to get the best choice. The value of n sub i is the size of all_hashes. As an example, if you chose k to be .001, we would get the following:\n",
    "<p>\n",
    "<code>\n",
    "P(#stevef | nohate) = (0 + .001)/(total_counts[0] + .001*23013)\n",
    "</code>\n",
    "<p>\n",
    "There are a lot of interesting variations on this style of smoothing (called Laplace Smoothing). And the cool thing is we have 2 faculty that do research with probabilistic reasoning. I encourage you to take a course from Daniel Lowd and Thanh Nguyen to get a deeper view into the problem. We just hired Thanh so you can also welcome her to the department!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Wrangling the table\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Before you leave, write the tweet_table out to file. We did a lot of wrangling on it so let's save what we have done.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week = '3' # change this each version\n",
    "\n",
    "home_path =  os.path.expanduser('~')\n",
    "\n",
    "file_path = '/Documents/datascience_2/table_history/'  #use your own path\n",
    "\n",
    "file_name = 'tweet_table_w'+week+'.csv'\n",
    "\n",
    "tweet_table.to_csv(home_path + file_path + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
