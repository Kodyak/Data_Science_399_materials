{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a3cb0ee3-7bca-4b2b-8a27-be198d18818e",
    "_uuid": "075ab0f3fc310e293828b3681f1d80642f88c106"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".h1_cell, .just_text {\n",
    "    box-sizing: border-box;\n",
    "    padding-top:5px;\n",
    "    padding-bottom:5px;\n",
    "    font-family: \"Times New Roman\", Georgia, Serif;\n",
    "    font-size: 125%;\n",
    "    line-height: 22px; /* 5px +12px + 5px */\n",
    "    text-indent: 25px;\n",
    "    background-color: #fbfbea;\n",
    "    padding: 10px;\n",
    "}\n",
    "\n",
    "hr { \n",
    "    display: block;\n",
    "    margin-top: 0.5em;\n",
    "    margin-bottom: 0.5em;\n",
    "    margin-left: auto;\n",
    "    margin-right: auto;\n",
    "    border-style: inset;\n",
    "    border-width: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Midterm 1\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "There are three questions (with sub-parts) on the exam. I'll give the majority of points to questions 1 and 2 so focus on those first. Then try your hand at question 3.\n",
    "<p>\n",
    "Please only use libraries that we have seen in prior modules. If you have a question about this, come up and ask me.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Question 1\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "This problem starts with bag_of_words from week 4. First thing you need to do is read that in.\n",
    "<p>\n",
    "The question has a part a and part b.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aaem', [1, 0, 0]),\n",
       " ('ab', [1, 0, 0]),\n",
       " ('aback', [2, 0, 0]),\n",
       " ('abaft', [0, 0, 1]),\n",
       " ('abandon', [7, 3, 1]),\n",
       " ('abandoned', [11, 13, 5]),\n",
       " ('abandoning', [2, 1, 0]),\n",
       " ('abandonment', [2, 0, 3]),\n",
       " ('abaout', [0, 24, 0]),\n",
       " ('abased', [1, 0, 0])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I am reading from file but you can load from url if you want to replace my code\n",
    "\n",
    "import json\n",
    "bag_of_words = json.load(open(\"bag_of_words.txt\"))  # assumes you have previously written it out\n",
    "sorted(bag_of_words.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Important words for each author\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "I'd like to know the words that \"stand out\" for each author. I am going to define stand out in a specific way. I'll tell you what I want and then ask you to code it.\n",
    "<p>\n",
    "First, let's clone bag_of_words so we have a new dictionary to work with. The keys will stay the same but we will be changing the values of each key.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "all_word_importance = copy.deepcopy(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "What I want for each word (key) in the `all_word_importance` dictionary is a new list of 3 values. Each item in this list reflects how important the word is to the respective author. Let's take, for example, the word `abandon`. It has a value in bag_of_words of \n",
    "`[7, 3, 1]`. It seems more imporant to EAP than to HPL or MWS. But how to compute this importance? Here is the general idea. For the word abandon, I compute its importance to EAP as follows:\n",
    "<p>\n",
    "<code>\n",
    "importance(abandon, EAP) = tf(abandon, EAP) `*` idf(abandon)\n",
    "</code>\n",
    "<p>\n",
    "Let's look at the 2 terms on the right hand side in more detail.\n",
    "<p>\n",
    "`tf(abandon, EAP)` = number of times abandon appears in EAP sentences. But wait. That is exactly what we have in `bag_of_words['abandon'][0]`. So that is easy. Second term next.\n",
    "<p>\n",
    "`idf(abandon) = log10(X)` where X is total number of authors divided by the total number of authors minus the number of authors who do *not* use abandon. You may notice we will get a 0 in denom if no authors use abandon, i.e., 3-3=0. But that is nonsensical - one author had to use the word abandon to get in the dictionary. You may also notice that idf does not reference the author. More on that in a minute. We have 3 authors so we have this formula for `idf` for word abandon:\n",
    "<p>\n",
    "`idf(abandon) = log10(3./(3-Z))`  where Z is number of 0 values in the abandon list, i.e., `[7, 3, 1]`.\n",
    "<p>\n",
    "Given that `Z` is `0` (the count of 0 values is zero) we have `log10(1.0)`. And that simplifies to 0. So `idf` for abandon is 0.\n",
    "<p>\n",
    "Importance of the word abandon for EAP is `tf(EAP)*idf`. That is what we put in the first list item for abandon in `all_word_importance`.\n",
    "<p>\n",
    "We now have calculated the value for EAP. We can use the same formula for HPL and MWS. They will each have new `tf` values but the same `idf` value. Hint: you only need to calculate `idf` once per word. When you have finished, you will have 3 values for `abandon` in `all_word_importance`, one for each author.\n",
    "<p>\n",
    "<code>\n",
    "importance(abandon, EAP) = tf(abandon, EAP) `*` idf(abandon)  # all_word_importance['abandon'][0] is 0\n",
    "importance(abandon, HPL) = tf(abandon, HPL) `*` idf(abandon)  # all_word_importance['abandon'][1] is 0\n",
    "importance(abandon, MWS) = tf(abandon, MWS) `*` idf(abandon)  # all_word_importance['abandon'][2] is 0\n",
    "</code>\n",
    "<p>\n",
    "<p>\n",
    "You can see that we will end up with the list [0,0,0] given `idf` is 0. In essence, no authors stands out in terms of the use of the word `abandon`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Part a: Define `calculate_importance`\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Go ahead and define the calculate_importance function and then test it against my results. I added a debug flag. When it is True, I print word_bag[word]. When it is False or omitted, no printing occurs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_importance(word_bag, word, author_list, debug=False):\n",
    "    \n",
    "    \n",
    "    # compute idf\n",
    "    word_users = 0\n",
    "    for i in range(len(author_list)):\n",
    "        if word_bag[word][i] == 0:\n",
    "            word_users += 1\n",
    "    #\n",
    "    X = len(author_list) / ( len(author_list) - word_users )\n",
    "    idf = math.log(X,10)\n",
    "    \n",
    "    importance = []\n",
    "    for i in range(len(author_list)):\n",
    "        # compute tf\n",
    "        tf = word_bag[word][i] * idf\n",
    "        importance.append(tf)\n",
    "    \n",
    "    if debug == True:\n",
    "        print(word_bag[word])\n",
    "    \n",
    "    return importance\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_list = [\"EAP\", \"HPL\", \"MWS\"]\n",
    "calculate_importance(bag_of_words, 'abandon', author_list, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "Let's try `abandoning`. You will see it does have a 0 count so `idf` is log10(3/(3-1)), i.e., non-zero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3521825181113625, 0.17609125905568124, 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_importance(bag_of_words, 'abandoning', author_list, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "<p>\n",
    "Twice as important to EAP than to HPL.\n",
    "<p>\n",
    "Here are a couple more.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47712125471966244, 0.0, 0.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_importance(bag_of_words, 'abased', author_list, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 24, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 11.450910113271899, 0.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_importance(bag_of_words, 'abaout', author_list, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=h1_cell>\n",
    "Let's build the entire dictionary for `all_word_importance`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in bag_of_words:\n",
    "    all_word_importance[word] = calculate_importance(bag_of_words, word, author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aaem', [0.47712125471966244, 0.0, 0.0]),\n",
       " ('ab', [0.47712125471966244, 0.0, 0.0]),\n",
       " ('aback', [0.9542425094393249, 0.0, 0.0]),\n",
       " ('abaft', [0.0, 0.0, 0.47712125471966244]),\n",
       " ('abandon', [0.0, 0.0, 0.0]),\n",
       " ('abandoned', [0.0, 0.0, 0.0]),\n",
       " ('abandoning', [0.3521825181113625, 0.17609125905568124, 0.0]),\n",
       " ('abandonment', [0.3521825181113625, 0.0, 0.5282737771670437]),\n",
       " ('abaout', [0.0, 11.450910113271899, 0.0]),\n",
       " ('abased', [0.47712125471966244, 0.0, 0.0])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_word_importance.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Part b: Show the top k most important words for each author\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "Write a function that will return the top `k` most important words for an author using the `all_word_importance` dictionary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def top_k_by_author(k, author, author_list):\n",
    "    \n",
    "    # get author index\n",
    "    author = [author] * len(author_list)\n",
    "    L = [int(x==y) for x,y in zip(author, author_list)]\n",
    "    \n",
    "    index = L.index(max(L))\n",
    "    \n",
    "    sorted_word_importance_list = sorted(all_word_importance.keys(), key=lambda x: all_word_importance[x][index], reverse=True )\n",
    "        \n",
    "    top_k = [ (x,all_word_importance[x]) for x in sorted_word_importance_list[0:k] ] \n",
    "    \n",
    "    return top_k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dupin', [27.67303277374042, 0.0, 0.0]),\n",
       " ('marie', [23.856062735983123, 0.0, 0.0]),\n",
       " ('jupiter', [21.47045646238481, 0.0, 0.0]),\n",
       " ('monsieur', [18.607728934066834, 0.0, 0.0]),\n",
       " ('ellison', [13.83651638687021, 0.0, 0.0]),\n",
       " ('color', [13.83651638687021, 0.0, 0.0]),\n",
       " ('maelzel', [13.83651638687021, 0.0, 0.0]),\n",
       " ('bug', [13.83651638687021, 0.0, 0.0]),\n",
       " ('prefect', [13.359395132150548, 0.0, 0.0]),\n",
       " ('madame', [13.030753170120411, 0.0, 0.5282737771670437])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_by_author(10, 'EAP', author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gilman', [0.0, 36.26121535869434, 0.0]),\n",
       " ('innsmouth', [0.0, 28.627275283179745, 0.0]),\n",
       " ('arkham', [0.0, 28.150154028460083, 0.0]),\n",
       " ('whateley', [0.0, 25.28742650014211, 0.0]),\n",
       " ('aout', [0.0, 19.56197144350616, 0.0]),\n",
       " ('folk', [0.0, 17.176365169907847, 0.0]),\n",
       " ('wilbur', [0.0, 16.699243915188184, 0.0]),\n",
       " ('armitage', [0.0, 16.22212266046852, 0.0]),\n",
       " ('carter', [0.0, 14.313637641589873, 0.0]),\n",
       " ('bearded', [0.0, 14.313637641589873, 0.0])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_by_author(10, 'HPL', author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('perdita', [0.0, 0.0, 80.63349204762295]),\n",
       " ('adrian', [0.0, 0.0, 67.2740969154724]),\n",
       " ('idris', [0.0, 0.0, 52.0062167644432]),\n",
       " ('raymond', [0.0, 0.3521825181113625, 47.544639945033936]),\n",
       " ('windsor', [0.0, 0.0, 34.829851594535356]),\n",
       " ('felix', [0.0, 0.0, 20.993335207665147]),\n",
       " ('justine', [0.0, 0.0, 20.039092698225822]),\n",
       " ('evadne', [0.0, 0.0, 19.084850188786497]),\n",
       " ('misery', [0.704365036222725, 0.0, 17.785217164623806]),\n",
       " ('clerval', [0.0, 0.0, 17.65348642462751])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_by_author(10, 'MWS', author_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Question 2\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Let's look at the inverse problem from question 1. In question 1 we were looking for the most unique words to a specific author. Now I want the words that are most common among all 3 authors. My idea is that if a word does not stand out in any way, maybe we should add that word to our stop words. Wrangle it out before we call on Naive Bayes\n",
    "<p>\n",
    "Here is how I am going to define common for a word W. First I'll get the 3 values from bag_of_words(W). I'll take the standard-deviation of the 3 values. If each value is within  k`*`sigma of the mean (sigma is standard deviation) then I'll count the word as a potential stop word: none of the values stand-out so not a useful word to differentiate authors.\n",
    "<p>\n",
    "Please write the standard-deviation function yourself - you can use math but nothing else.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Part a: define `check_common`\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Write a function `check_common` that takes a vector (the list of 3 values) as argument and the value of k as an argument. It returns True if each value is within k*sigma of the mean.\n",
    "<p>One final spec: if you see a value of 0 in any place, then return False. If a word has at least one 0 value, it must be good at doing some differentiation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(Data):\n",
    "    return sum(Data)/ len(Data)\n",
    "\n",
    "\n",
    "def standard_deviation(Data):\n",
    "    \n",
    "    diffs_squared = [(x - mean(Data))**2 for x in Data]\n",
    "    return math.sqrt( sum(diffs_squared) / (len(Data)-1) )\n",
    "\n",
    "def check_common(vect, k):\n",
    "    \n",
    "    for i in range(len(vect)):\n",
    "        if vect[i] == 0:\n",
    "            return False\n",
    "    \n",
    "    veracity = [ (x <= mean(vect) + k*standard_deviation(vect))  \n",
    "                & (x >= mean(vect) - k*standard_deviation(vect)) for x in vect]\n",
    "    if veracity == [True]*len(vect):\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_common([3,3,3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_common([3,4,4],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_common([3,3,4],1.3)  # notice using 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_common([3,3,100],1.2) # notice using 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Part b: build a list of common words\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Build `common_words_1` to contain all the words within one sigma.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-14d2a5882b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mcommon_words_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "common_words_1 = []\n",
    "\n",
    "for words in bag_of_words.items():\n",
    "    if check_common(bag_of_words[words], 1):\n",
    "        common_words_1.append(words)\n",
    "\n",
    "len(common_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 1]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words['process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'hanging', [5, 5, 5])\n",
      "(u'climber', [1, 1, 1])\n",
      "(u'perfumed', [2, 2, 2])\n",
      "(u'undergone', [2, 2, 2])\n",
      "(u'keenly', [1, 1, 1])\n",
      "(u'jargon', [1, 1, 1])\n",
      "(u'substances', [1, 1, 1])\n",
      "(u'purport', [1, 1, 1])\n",
      "(u'extorted', [1, 1, 1])\n",
      "(u'retarded', [1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for word in common_words_1[:10]:\n",
    "    print((word, bag_of_words[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Part c: check my hypothesis\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "It looks to me like all words in common_words_1 have equal frequencies. Please check out my hypothesis. Show me whether I am right or wrong.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Print out any word in common_words_1 that does not have 3 equal counts - you should not see any printouts\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Part d: build another list of common words\n",
    "</h2>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Build `common_words_1_3` to contain all the words within 1.3 `*` sigma.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(common_words_1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'woods', [7, 18, 28])\n",
      "(u'hanging', [5, 5, 5])\n",
      "(u'climber', [1, 1, 1])\n",
      "(u'stern', [11, 7, 3])\n",
      "(u'circumstances', [59, 8, 37])\n",
      "(u'locked', [9, 16, 4])\n",
      "(u'want', [14, 20, 26])\n",
      "(u'travel', [1, 6, 4])\n",
      "(u'idle', [5, 7, 3])\n",
      "(u'tempted', [4, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "for word in common_words_1_3[:10]:\n",
    "    print((word, bag_of_words[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Question 3\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "I would like to find interrogatory sentences within the gothic sentences we have been working with. My theory is that certain authors use more questions than others. I want to break it down further. I will assume a question starts with the type of question it is, e.g., Who, What, Why, How, etc. I would like to know the first word in an interrogatory sentence. I also assume an interrogatory sentence ends with a question mark.\n",
    "<p>\n",
    "Your goal is to define mpat below so you match my results. Please don't change my testing code.\n",
    "<p>\n",
    "I am looking for a general pattern. If you supply a pattern that only works with my test sentences, I consider that over-fitting.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is it going? huh\n",
      "no match\n",
      "==========\n",
      "why are you going?\n",
      "why\n",
      "==========\n",
      "When is not the question.\n",
      "no match\n",
      "==========\n",
      "When?\n",
      "When\n",
      "==========\n",
      "What the heck??\n",
      "What\n",
      "==========\n",
      "What does ? mean in a pattern?\n",
      "What\n",
      "==========\n",
      "?\n",
      "no match\n",
      "==========\n",
      "??\n",
      "no match\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "#my testing code - please do not change\n",
    "\n",
    "foo = 'How is it going? huh'\n",
    "fum = 'why are you going?'\n",
    "fie = 'When is not the question.'\n",
    "foe = 'When?'\n",
    "fee = 'What the heck??'\n",
    "fah = 'What does ? mean in a pattern?'\n",
    "far = '?'\n",
    "fab = '??'\n",
    "for sentence in [foo, fum, fie, foe, fee, fah, far, fab]:\n",
    "    print(sentence)\n",
    "    \n",
    "    m = re.match(mpat, sentence)\n",
    "    if m == None:\n",
    "        print('no match')\n",
    "    else:\n",
    "         print(m.groupdict()['qtype'])\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
